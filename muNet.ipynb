{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VstOm6tOFPRO",
        "GW_4lkyANWk-"
      ],
      "authorship_tag": "ABX9TyOCZA0yBR0dHdfq6paqKSfS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmandaJMendes/muNet/blob/main/muNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "VstOm6tOFPRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"one_hot_cancer.csv\")\n",
        "data.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "n_20 = int(len(data)*0.2)\n",
        "test = data.sample(n = n_20)\n",
        "train_val = data.drop(test.index)\n",
        "val = train_val.sample(n = n_20)\n",
        "train = train_val.drop(val.index)\n",
        "x_test = test.iloc[:, :-2]\n",
        "y_test = test.iloc[:, -2]\n",
        "x_val = val.iloc[:, :-2]\n",
        "y_val = val.iloc[:, -2]\n",
        "x_train = train.iloc[:, :-2]\n",
        "y_train = train.iloc[:, -2]"
      ],
      "metadata": {
        "id": "m-Ub0tx7FQqb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv(\"Breast_Cancer.csv\")\n",
        "data2.rename(columns={'T Stage ': 'T Stage'}, inplace=True)\n",
        "ndata = data2.copy()\n",
        "\n",
        "for i in [3, 4, 5, 8, 10, 11, 15]:\n",
        "  ndata.iloc[:, i] = ndata.iloc[:, i].astype(\"category\").cat.codes\n",
        "ndata.iloc[:, 6] = ndata.iloc[:, 6].astype(\"category\").cat.set_categories(['Undifferentiated',\n",
        "                                                                           'Poorly differentiated',\n",
        "                                                                           'Moderately differentiated',\n",
        "                                                                           'Well differentiated'],\n",
        "                                                                          ordered = True).cat.codes\n",
        "\n",
        "ndata.iloc[:, 7] = ndata.iloc[:, 7].astype(\"category\").cat.set_categories(['1', '2', '3', ' anaplastic; Grade IV'],\n",
        "                                                                          ordered = True).cat.codes\n",
        "\n",
        "ndata = pd.get_dummies(ndata, columns = [\"Race\", \"Marital Status\"])\n",
        "ndata = ndata.astype(\"int64\")  \n",
        "n_20_2 = int(len(ndata)*0.2)\n",
        "test2 = ndata.sample(n = n_20)\n",
        "train_val2 = ndata.drop(test2.index)\n",
        "val2 = train_val2.sample(n = n_20_2)\n",
        "train2 = train_val2.drop(val2.index)\n",
        "x_test2 = test2.iloc[:, :-2]\n",
        "y_test2 = test2.iloc[:, -2]\n",
        "x_val2 = val2.iloc[:, :-2]\n",
        "y_val2 = val2.iloc[:, -2]\n",
        "x_train2 = train2.iloc[:, :-2]\n",
        "y_train2 = train2.iloc[:, -2]"
      ],
      "metadata": {
        "id": "xDLVFo_tVV7E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create root model"
      ],
      "metadata": {
        "id": "5pAOqBDP8b1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "ddYevXhdB3YX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import load_model, clone_model\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "        Flatten(input_shape=(39,)),\n",
        "        Dense(32, activation=relu),\n",
        "        Dense(32, activation=relu),\n",
        "\t      Dense(16, activation=relu),\n",
        "        Dense(8, activation=relu),\n",
        "        Dense(1, activation=sigmoid),])\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "model.save(\"root_model.h5\")"
      ],
      "metadata": {
        "id": "igOAjhB985Qr"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MuNet\n",
        "Attempt to clone optimizer state\n",
        "(I guess it works, but I'm not happy with it)"
      ],
      "metadata": {
        "id": "Vq1HHZNMn4XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class muNet:\n",
        "  def __init__(self):\n",
        "    \n",
        "    self.tasks = [{\"x_train\": x_train, \"y_train\": y_train,\n",
        "                   \"x_val\": x_val, \"y_val\": y_val},\n",
        "                  {\"x_train\": x_train2, \"y_train\": y_train2,\n",
        "                   \"x_val\": x_val2, \"y_val\": y_val2}]\n",
        "\n",
        "    self.n = len(self.tasks)\n",
        "    self.root_model = load_model(\"/content/root_model.h5\")\n",
        "    \n",
        "    cols1 = pd.MultiIndex.from_tuples([(\"Model\", \"\"),\n",
        "                                       (\"Learning Rate\", \"\"),\n",
        "                                       (\"Batch Size\", \"\")])\n",
        "    cols2 = pd.MultiIndex.from_product([['Selected','Score'],\n",
        "                                        [f'T{i}' for i in range(self.n)]])\n",
        "    data1 = [[self.root_model, 0.01, 128]]\n",
        "    data2 = [[0 for i in range(self.n)]+[-1.0 for i in range(self.n)]]\n",
        "\n",
        "    self.models = pd.concat([pd.DataFrame(data1, columns = cols1),\n",
        "                             pd.DataFrame(data2, columns = cols2)], axis=1)\n",
        "    \n",
        "    self.layers = pd.DataFrame([[l, l.name]+4*[None]+[[]] for l in self.root_model.layers],\n",
        "                               columns = [\"Layer\", \"Name\", \"Kernel_m\", \"Bias_m\",\n",
        "                                          \"Kernel_v\", \"Bias_v\", \"Models\"])\n",
        "    self.layer_name = int(self.layers.tail(1).Name.values[0].split(\"_\")[-1])\n",
        "    self.active_pop = []\n",
        "    self.active_task = None\n",
        "    self.mutation_options = {\"Learning Rate\": [0.0001, 0.0002, 0.0005,\n",
        "                                               0.001, 0.002, 0.005,\n",
        "                                               0.01, 0.02, 0.05, 0.1],\n",
        "                             \"Batch Size\": [1, 2, 4, 8, 16, 32, 64, 128, 256]}\n",
        "\n",
        "  def sample_parent(self):\n",
        "    active = self.models.loc[self.active_pop, :]\n",
        "    sorted_active = list(active.sort_values((\"Score\", f\"T{self.active_task}\")).index)\n",
        "    others = list(self.models.drop(self.active_pop).index)\n",
        "    np.random.shuffle(others)\n",
        "    options = sorted_active + others\n",
        "\n",
        "    parent = None\n",
        "    for model_idx in options:\n",
        "      selections = self.models.loc[model_idx, (\"Selected\", f\"T{self.active_task}\")]\n",
        "      if 0.5**selections > np.random.uniform():\n",
        "        parent = model_idx\n",
        "        break\n",
        "    if not parent:\n",
        "      parent = np.random.choice(options)\n",
        "\n",
        "    self.models.at[parent, (\"Selected\", f\"T{self.active_task}\")] += 1\n",
        "    return parent\n",
        "\n",
        "  def sample_mutations(self, parent):\n",
        "    new_hyper_params = {}\n",
        "\n",
        "    new_hyper_params[\"Clone\"] = []\n",
        "    for layer in range(len(self.models.loc[parent, (\"Model\", \"\")].layers)):\n",
        "      if np.random.uniform() > 0.5:\n",
        "        print(\"clone\")\n",
        "        new_hyper_params[\"Clone\"].append(layer)\n",
        "\n",
        "    for key, options in self.mutation_options.items():\n",
        "      current_hp = self.models.loc[parent, (key, \"\")] \n",
        "      change = 1 if np.random.uniform() > 0.5 else 0\n",
        "      print(change)\n",
        "      if change:\n",
        "        current_hyperparam_idx = options.index(current_hp)\n",
        "        if current_hyperparam_idx == 0:\n",
        "          new_hyperparam_idx = current_hyperparam_idx + 1\n",
        "        elif current_hyperparam_idx == len(options)-1:\n",
        "          new_hyperparam_idx = current_hyperparam_idx - 1\n",
        "        else:\n",
        "          new_hyperparam_idx = current_hyperparam_idx + 1 if np.random.uniform() > 0.5 else current_hyperparam_idx - 1\n",
        "\n",
        "        new_hyper_params[key] = options[new_hyperparam_idx]\n",
        "      else:\n",
        "        new_hyper_params[key] = current_hp\n",
        "    return new_hyper_params\n",
        "\n",
        "  def clone_layer(self, layer, change = None, in_shape = None, clone_w = False):\n",
        "    print(layer.name, change, in_shape, clone_w)\n",
        "    config = layer.get_config()\n",
        "    for param, value in change.items():\n",
        "      config[param] = value\n",
        "    clone = type(layer).from_config(config)\n",
        "    clone.build(in_shape if in_shape else layer.input_shape)\n",
        "    if clone_w:\n",
        "      clone.set_weights(layer.get_weights())\n",
        "    clone.trainable = True    \n",
        "    return clone\n",
        "\n",
        "  def append_to_df(self, dataframe, rows):\n",
        "    new_df = pd.DataFrame(rows, columns = dataframe.columns)\n",
        "    return pd.concat([dataframe, new_df]).reset_index(drop=True)\n",
        "\n",
        "  def create_name(self, name):\n",
        "    self.layer_name+=1\n",
        "    return ''.join([i for i in name if not i.isdigit()])+str(self.layer_name)\n",
        "\n",
        "  def create_child(self, parent, mutations):\n",
        "    parent_model = self.models.loc[parent, (\"Model\", \"\")]\n",
        "    task_in_shape = self.tasks[self.active_task][\"x_train\"].shape\n",
        "    task_out_shape = (None, 1)\n",
        "\n",
        "    to_clone = mutations[\"Clone\"]\n",
        "    if 0 in to_clone:\n",
        "      to_clone.remove(0)\n",
        "    child_layers = []\n",
        "    for layer_idx, layer in enumerate(parent_model.layers):\n",
        "      print(\"layer name: \", self.layer_name)\n",
        "      print(task_in_shape, layer.input_shape)\n",
        "      if layer_idx == 0 and task_in_shape[1] != layer.input_shape[1]:\n",
        "        clone = self.clone_layer(layer,\n",
        "                                 change = {\"name\": self.create_name(layer.name)},\n",
        "                                 in_shape = (task_in_shape[-1], ))\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      elif layer_idx == 1 and task_in_shape[-1] != layer.input_shape[1]:\n",
        "        clone = self.clone_layer(layer,\n",
        "                                 change = {\"name\": self.create_name(layer.name)},\n",
        "                                 in_shape = (None, task_in_shape[-1]))\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      elif layer_idx == len(parent_model.layers)-1:\n",
        "        print(layer.output.shape, task_out_shape)\n",
        "        if layer.output.shape == task_out_shape:\n",
        "          clone = self.clone_layer(layer,\n",
        "                                   change = {\"name\": self.create_name(layer.name)},\n",
        "                                   clone_w = True)\n",
        "        else:\n",
        "          clone = self.clone_layer(layer,\n",
        "                                   change = {\"units\": task_out_shape[-1],\n",
        "                                             \"name\": self.create_name(layer.name)})\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      elif layer_idx in mutations[\"Clone\"]:#or parent_model == self.root_model:\n",
        "        clone = self.clone_layer(layer, change = {\"name\": self.create_name(layer.name)}, clone_w = True)\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      else:\n",
        "        layer.trainable = False\n",
        "        child_layers.append(layer)\n",
        "   \n",
        "    child_model = Sequential(child_layers)\n",
        "    child_model.compile(optimizer=optimizers.Adam(learning_rate=mutations[\"Learning Rate\"]),\n",
        "                        loss='binary_crossentropy',\n",
        "                        metrics=['acc'])\n",
        "    return child_model\n",
        "    \"\"\"\n",
        "    grad_vars = child_model.trainable_weights\n",
        "    zero_grads = [tf.zeros_like(w) for w in grad_vars]\n",
        "    child_model.optimizer.apply_gradients(zip(zero_grads, grad_vars))    \n",
        "    ms = []\n",
        "    vs = []\n",
        "    for layer in [l for l in child_model.layers if l.trainable]:\n",
        "      layer_df = self.layers.loc[self.layers.Layer == layer]\n",
        "      if layer_df.Kernel_m.values[0]:\n",
        "        ms.append(layer_df.Kernel_m.values[0])\n",
        "        ms.append(layer_df.Bias_m.values[0])\n",
        "        vs.append(layer_df.Kernel_v.values[0])\n",
        "        vs.append(layer_df.Bias_v.values[0])\n",
        "      else:\n",
        "        if len(layer.get_weights()):\n",
        "          ms.append(tf.zeros_like(layer.get_weights()[0]))\n",
        "          ms.append(tf.zeros_like(layer.get_weights()[1]))\n",
        "          vs.append(tf.zeros_like(layer.get_weights()[0]))\n",
        "          vs.append(tf.zeros_like(layer.get_weights()[1]))\n",
        "    print([i.shape for i in child_model.optimizer.get_weights()])\n",
        "    print([i.shape for i in ms])\n",
        "    print([i.shape for i in vs])\n",
        "    child_model.optimizer.set_weights([tf.Variable(0.0)]+ms+vs)\n",
        "    return child_model\n",
        "    \"\"\"\n",
        "  def train_child(self, child, bs):\n",
        "    child.fit(self.tasks[self.active_task][\"x_train\"],\n",
        "              self.tasks[self.active_task][\"y_train\"],\n",
        "              epochs = 1,\n",
        "              batch_size = bs)\n",
        "    return child\n",
        "\n",
        "  def score(self, model):\n",
        "    loss, acc = model.evaluate(self.tasks[self.active_task][\"x_val\"],\n",
        "                               self.tasks[self.active_task][\"y_val\"])\n",
        "    return acc\n",
        "  \n",
        "  def evaluate(self, row, x_test, y_test):\n",
        "    try:\n",
        "      loss, acc = self.models.iloc[row, 0].evaluate(x_test, y_test)\n",
        "    except RuntimeError:\n",
        "      self.models.iloc[row,0].compile(optimizer=optimizers.Adam(),\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['acc'])\n",
        "      loss, acc = self.models.iloc[row,0].evaluate(x_test, y_test)\n",
        "\n",
        "    return self.models.iloc[[row]], acc, loss\n",
        "  \n",
        "  def train_system(self, generations, children):\n",
        "    #Treinar mais de uma vez\n",
        "    for cycle in range(4):\n",
        "      for task in range(2):\n",
        "        print(\"Task: \", task)\n",
        "        self.active_task = task\n",
        "        self.active_pop = list(self.models[self.models[(\"Score\", f\"T{task}\")] >= 0].index)\n",
        "        print(\"Active pop: \", self.active_pop)\n",
        "\n",
        "        for gen in range(generations):\n",
        "          for child in range(children):\n",
        "            #Tentar manter apenas a melhor child da geração e não deixar que modelos da mesma geração sejam pais e filhos\n",
        "            self.counter+=1\n",
        "            parent = self.sample_parent()\n",
        "            print(\"Parent: \", parent)\n",
        "            mutations = self.sample_mutations(parent)\n",
        "            print(\"Mutations: \", mutations)\n",
        "            child = self.create_child(parent, mutations)\n",
        "            print(\"Child layers: \", [i.trainable for i in child.layers])\n",
        "            \n",
        "            retained_child = None\n",
        "            best_score = self.models.loc[parent, (\"Score\", f\"T{task}\")]\n",
        "            for i in range(4):\n",
        "              print(\"Best score: \", best_score)\n",
        "              child = self.train_child(child, mutations[\"Batch Size\"])\n",
        "              print(\"Child: \", child)\n",
        "              child_score = self.score(child)\n",
        "              print(\"Child score: \", child_score)\n",
        "              if child_score > best_score:\n",
        "                best_score = child_score\n",
        "\n",
        "                retained_child = clone_model(child)\n",
        "                retained_child.set_weights(child.get_weights()) \n",
        "                symbolic_weights = getattr(child.optimizer, 'weights') \n",
        "                retained_opt = K.batch_get_value(symbolic_weights) \n",
        "                retained_child.compile(optimizer=optimizers.Adam(),\n",
        "                                        loss='binary_crossentropy',\n",
        "                                        metrics=['acc'])\n",
        "                grad_vars = child.trainable_weights\n",
        "                zero_grads = [tf.zeros_like(w) for w in grad_vars]\n",
        "                retained_child.optimizer.apply_gradients(zip(zero_grads, grad_vars))\n",
        "                retained_child.optimizer.set_weights(retained_opt)\n",
        "                \n",
        "              \n",
        "            if retained_child:\n",
        "              k = 1\n",
        "              new_rows = []\n",
        "              tls = [l for l in retained_child.layers if l.trainable]\n",
        "              if type(tls[0]) == Flatten:\n",
        "                tls = tls[1:]\n",
        "              for layer in tls:\n",
        "                if not type(layer) == Flatten:\n",
        "                  layer_row = self.layers[self.layers.Layer == layer]\n",
        "                  self.layers.drop(layer_row.index)\n",
        "                  print([a.shape for a in retained_child.optimizer.get_weights()])\n",
        "                  print(k)\n",
        "                  new_rows.append([layer,\n",
        "                                  retained_child.optimizer.get_weights()[k],\n",
        "                                  retained_child.optimizer.get_weights()[k+1],\n",
        "                                  retained_child.optimizer.get_weights()[k+len(tls)*2],\n",
        "                                  retained_child.optimizer.get_weights()[k+len(tls)*2+1],\n",
        "                                  []])\n",
        "                  k += 2\n",
        "              self.layers = pd.concat([self.layers,\n",
        "                                       pd.DataFrame(new_rows, columns = self.layers.columns)])\n",
        "              self.models = pd.concat([self.models,\n",
        "                                      pd.DataFrame([[retained_child] + \\\n",
        "                                                    [mutations[\"Learning Rate\"], mutations[\"Batch Size\"]] + \\\n",
        "                                                    [0 for i in range(self.n)]+[-1.0 for i in range(self.n)]],\n",
        "                                                    columns = self.models.columns)]).reset_index(drop=True)                                     \n",
        "              self.models.iat[-1, self.models.columns.get_loc((\"Score\", f\"T{task}\"))] = best_score\n",
        "              self.active_pop.append(self.models.index[-1])\n",
        "\n",
        "              \n",
        "        if self.active_pop:\n",
        "          print(\"Active pop end: \", self.active_pop)\n",
        "          active_pop_scores = self.models.loc[self.active_pop, (\"Score\", f\"T{task}\")]\n",
        "          best_row = self.models.loc[[active_pop_scores.idxmax()]]\n",
        "\n",
        "          not_trained = self.models.drop(active_pop_scores.index)\n",
        "\n",
        "          self.models = pd.concat([not_trained, best_row]).reset_index(drop=True)\n",
        "          print(\"Models end: \", self.models)\n",
        "          #Ideia: não manter só o melhor. Criar métrica que considera se está acima de um threshold e já foi usado como pai tantas vezes\n",
        "        print(\"Models at the end of task:\\n\", self.models)"
      ],
      "metadata": {
        "id": "MjLTV41Vf5Mu"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MuNet (newest version)"
      ],
      "metadata": {
        "id": "NcaNgIVEol23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class muNet:\n",
        "  def __init__(self):\n",
        "    \n",
        "    self.tasks = [{\"x_train\": x_train, \"y_train\": y_train,\n",
        "                   \"x_val\": x_val, \"y_val\": y_val},\n",
        "                  {\"x_train\": x_train2, \"y_train\": y_train2,\n",
        "                   \"x_val\": x_val2, \"y_val\": y_val2}]\n",
        "\n",
        "    self.n = len(self.tasks)\n",
        "    self.root_model = load_model(\"/content/root_model.h5\")\n",
        "    \n",
        "    cols1 = pd.MultiIndex.from_tuples([(\"Model\", \"\"),\n",
        "                                       (\"Learning Rate\", \"\"),\n",
        "                                       (\"Batch Size\", \"\")])\n",
        "    cols2 = pd.MultiIndex.from_product([['Selected','Score'],\n",
        "                                        [f'T{i}' for i in range(self.n)]])\n",
        "    data1 = [[self.root_model, 0.01, 128]]\n",
        "    data2 = [[0 for i in range(self.n)]+[-1.0 for i in range(self.n)]]\n",
        "\n",
        "    self.models = pd.concat([pd.DataFrame(data1, columns = cols1),\n",
        "                             pd.DataFrame(data2, columns = cols2)], axis=1)\n",
        "    self.layers = pd.DataFrame([[l, l.name]+4*[None]+[[]] for l in self.root_model.layers],\n",
        "                               columns = [\"Layer\", \"Name\", \"Kernel_m\", \"Bias_m\",\n",
        "                                          \"Kernel_v\", \"Bias_v\", \"Models\"])\n",
        "    self.layer_name = int(self.layers.tail(1).Name.values[0].split(\"_\")[-1])\n",
        "    self.active_pop = []\n",
        "    self.active_task = None\n",
        "    self.mutation_options = {\"Learning Rate\": [0.0001, 0.0002, 0.0005,\n",
        "                                               0.001, 0.002, 0.005,\n",
        "                                               0.01, 0.02, 0.05, 0.1],\n",
        "                             \"Batch Size\": [1, 2, 4, 8, 16, 32, 64, 128, 256]}\n",
        "\n",
        "  def sample_parent(self):\n",
        "    active = self.models.loc[self.active_pop, :]\n",
        "    sorted_active = list(active.sort_values((\"Score\", f\"T{self.active_task}\")).index)\n",
        "    others = list(self.models.drop(self.active_pop).index)\n",
        "    np.random.shuffle(others)\n",
        "    options = sorted_active + others\n",
        "\n",
        "    parent = None\n",
        "    for model_idx in options:\n",
        "      selections = self.models.loc[model_idx, (\"Selected\", f\"T{self.active_task}\")]\n",
        "      if 0.5**selections > np.random.uniform():\n",
        "        parent = model_idx\n",
        "        break\n",
        "    if not parent:\n",
        "      parent = np.random.choice(options)\n",
        "\n",
        "    self.models.at[parent, (\"Selected\", f\"T{self.active_task}\")] += 1\n",
        "    return parent\n",
        "\n",
        "  def sample_mutations(self, parent):\n",
        "    new_hyper_params = {}\n",
        "\n",
        "    new_hyper_params[\"Clone\"] = []\n",
        "    for layer in range(1, len(self.models.loc[parent, (\"Model\", \"\")].layers)):\n",
        "      if np.random.uniform() > 0.5:\n",
        "        new_hyper_params[\"Clone\"].append(layer)\n",
        "\n",
        "    for key, options in self.mutation_options.items():\n",
        "      current_hp = self.models.loc[parent, (key, \"\")] \n",
        "      change = 1 if np.random.uniform() > 0.5 else 0\n",
        "      if change:\n",
        "        current_hyperparam_idx = options.index(current_hp)\n",
        "        if current_hyperparam_idx == 0:\n",
        "          new_hyperparam_idx = current_hyperparam_idx + 1\n",
        "        elif current_hyperparam_idx == len(options)-1:\n",
        "          new_hyperparam_idx = current_hyperparam_idx - 1\n",
        "        else:\n",
        "          new_hyperparam_idx = current_hyperparam_idx + 1 if np.random.uniform() > 0.5 else current_hyperparam_idx - 1\n",
        "\n",
        "        new_hyper_params[key] = options[new_hyperparam_idx]\n",
        "      else:\n",
        "        new_hyper_params[key] = current_hp\n",
        "    return new_hyper_params\n",
        "\n",
        "  def clone_layer(self, layer, change = None, in_shape = None, clone_w = False):\n",
        "\n",
        "    config = layer.get_config()\n",
        "    for param, value in change.items():\n",
        "      config[param] = value\n",
        "    clone = type(layer).from_config(config)\n",
        "    clone.build(in_shape if in_shape else layer.input_shape)\n",
        "    if clone_w:\n",
        "      clone.set_weights(layer.get_weights())\n",
        "    clone.trainable = True  \n",
        "    if type(layer)==Flatten:\n",
        "      print(layer.get_config())\n",
        "      print(config)  \n",
        "      print(in_shape)\n",
        "    return clone\n",
        "\n",
        "  def append_to_df(self, dataframe, rows):\n",
        "    new_df = pd.DataFrame(rows, columns = dataframe.columns)\n",
        "    return pd.concat([dataframe, new_df]).reset_index(drop=True)\n",
        "\n",
        "  def create_name(self, name):\n",
        "    self.layer_name+=1\n",
        "    return ''.join([i for i in name if not i.isdigit()])+str(self.layer_name)\n",
        "\n",
        "  def create_child(self, parent, mutations):\n",
        "    parent_model = self.models.loc[parent, (\"Model\", \"\")]\n",
        "    task_in_shape = self.tasks[self.active_task][\"x_train\"].shape\n",
        "    task_out_shape = (None, 1)\n",
        "    to_clone = mutations[\"Clone\"]\n",
        "    child_layers = []\n",
        "\n",
        "    for layer_idx, layer in enumerate(parent_model.layers):\n",
        "      if layer_idx == 0 and task_in_shape[1] != layer.input_shape[1]:\n",
        "        clone = self.clone_layer(layer,\n",
        "                                 change = {\"name\": self.create_name(layer.name),\n",
        "                                           \"batch_input_shape\": (None, task_in_shape[-1])},\n",
        "                                 in_shape = (task_in_shape[-1], ))\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      elif layer_idx == 1 and task_in_shape[-1] != layer.input_shape[1]:\n",
        "        clone = self.clone_layer(layer,\n",
        "                                 change = {\"name\": self.create_name(layer.name)},\n",
        "                                 in_shape = (None, task_in_shape[-1]))\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      elif layer_idx == len(parent_model.layers)-1:\n",
        "        if layer.output.shape == task_out_shape:\n",
        "          clone = self.clone_layer(layer,\n",
        "                                   change = {\"name\": self.create_name(layer.name)},\n",
        "                                   clone_w = True)\n",
        "        else:\n",
        "          clone = self.clone_layer(layer,\n",
        "                                   change = {\"units\": task_out_shape[-1],\n",
        "                                             \"name\": self.create_name(layer.name)})\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      elif layer_idx in mutations[\"Clone\"]:\n",
        "        clone = self.clone_layer(layer, change = {\"name\": self.create_name(layer.name)}, clone_w = True)\n",
        "        child_layers.append(clone)\n",
        "        self.layers = self.append_to_df(self.layers, [[clone, clone.name]+4*[None]+[[]]])\n",
        "\n",
        "      else:\n",
        "        layer.trainable = False\n",
        "        child_layers.append(layer)\n",
        "   \n",
        "    child_model = Sequential(child_layers)\n",
        "    child_model.compile(optimizer=optimizers.Adam(learning_rate=mutations[\"Learning Rate\"]),\n",
        "                        loss='binary_crossentropy',\n",
        "                        metrics=['acc'])\n",
        "    return child_model\n",
        "\n",
        "  def train_child(self, child, bs):\n",
        "    child.fit(self.tasks[self.active_task][\"x_train\"],\n",
        "              self.tasks[self.active_task][\"y_train\"],\n",
        "              epochs = 1,\n",
        "              batch_size = bs)\n",
        "    return child\n",
        "\n",
        "  def score(self, model):\n",
        "    loss, acc = model.evaluate(self.tasks[self.active_task][\"x_val\"],\n",
        "                               self.tasks[self.active_task][\"y_val\"])\n",
        "    return acc\n",
        "  \n",
        "  def train_system(self, generations, children):\n",
        "    for cycle in range(4):\n",
        "      print(\"Cycle \", cycle)\n",
        "      for task in range(2):\n",
        "        print(\"Task \", task)\n",
        "        self.active_task = task\n",
        "        self.active_pop = list(self.models[self.models[(\"Score\", f\"T{task}\")] >= 0].index)    \n",
        "        for gen in range(generations):\n",
        "          for child in range(children):\n",
        "            print(\"Active pop: \", self.active_pop)\n",
        "            parent = self.sample_parent()\n",
        "            print(\"Parent: \", [l.name for l in self.models.loc[parent, (\"Model\", \"\")].layers])\n",
        "            mutations = self.sample_mutations(parent)\n",
        "            child = self.create_child(parent, mutations)\n",
        "            print(\"Child: \", [l.name for l in child.layers])\n",
        "            retained_child = None\n",
        "            best_score = self.models.loc[parent, (\"Score\", f\"T{task}\")]\n",
        "            for i in range(4):\n",
        "              child = self.train_child(child, mutations[\"Batch Size\"])\n",
        "              child_score = self.score(child)\n",
        "\n",
        "              if child_score > best_score:\n",
        "                best_score = child_score\n",
        "                retained_child = clone_model(child)\n",
        "                retained_child.set_weights(child.get_weights()) \n",
        "                retained_child.compile(optimizer=optimizers.Adam(),\n",
        "                                        loss='binary_crossentropy',\n",
        "                                        metrics=['acc'])\n",
        "                \n",
        "                symbolic_weights = getattr(child.optimizer, 'weights') \n",
        "                retained_opt = K.batch_get_value(symbolic_weights) \n",
        "                grad_vars = child.trainable_weights\n",
        "                zero_grads = [tf.zeros_like(w) for w in grad_vars]\n",
        "                retained_child.optimizer.apply_gradients(zip(zero_grads, grad_vars))\n",
        "                retained_child.optimizer.set_weights(retained_opt)\n",
        "                         \n",
        "            if retained_child:\n",
        "              self.models = pd.concat([self.models,\n",
        "                                      pd.DataFrame([[retained_child] + \\\n",
        "                                                    [mutations[\"Learning Rate\"], mutations[\"Batch Size\"]] + \\\n",
        "                                                    [0 for i in range(self.n)]+[-1.0 for i in range(self.n)]],\n",
        "                                                    columns = self.models.columns)]).reset_index(drop=True)                                     \n",
        "              self.models.iat[-1, self.models.columns.get_loc((\"Score\", f\"T{task}\"))] = best_score\n",
        "              self.active_pop.append(self.models.index[-1])\n",
        "              \n",
        "        if self.active_pop:\n",
        "          active_pop_scores = self.models.loc[self.active_pop, (\"Score\", f\"T{task}\")]\n",
        "          best_row = self.models.loc[[active_pop_scores.idxmax()]]\n",
        "          not_trained = self.models.drop(active_pop_scores.index)\n",
        "          self.models = pd.concat([not_trained, best_row]).reset_index(drop=True)\n",
        "          print(\"Models (task end): \", self.models)\n"
      ],
      "metadata": {
        "id": "mUleMEb0dxRO"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mtl = muNet()\n",
        "mtl.train_system(2, 2)"
      ],
      "metadata": {
        "id": "KYHuMQfw0fIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f7e15e-49c3-446d-be4f-c0311dec8a43"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cycle  0\n",
            "Task  0\n",
            "Active pop:  []\n",
            "Parent:  ['flatten_5', 'dense_25', 'dense_26', 'dense_27', 'dense_28', 'dense_29']\n",
            "Child:  ['flatten_5', 'dense_25', 'dense_26', 'dense_27', 'dense_30', 'dense_31']\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4983 - acc: 0.8746\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3873 - acc: 0.8694\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3446 - acc: 0.8837\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3293 - acc: 0.8769\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3082 - acc: 0.8874\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3271 - acc: 0.8806\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3014 - acc: 0.8899\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3197 - acc: 0.8806\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_5', 'dense_25', 'dense_26', 'dense_27', 'dense_30', 'dense_31']\n",
            "Child:  ['flatten_5', 'dense_32', 'dense_26', 'dense_27', 'dense_33', 'dense_34']\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3062 - acc: 0.8895\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3182 - acc: 0.8843\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2882 - acc: 0.8978\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3125 - acc: 0.8893\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2758 - acc: 0.8978\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3018 - acc: 0.8918\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2674 - acc: 0.8994\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2978 - acc: 0.8905\n",
            "Active pop:  [1, 2]\n",
            "Parent:  ['flatten_5', 'dense_25', 'dense_26', 'dense_27', 'dense_30', 'dense_31']\n",
            "Child:  ['flatten_5', 'dense_25', 'dense_26', 'dense_35', 'dense_30', 'dense_36']\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3420 - acc: 0.8700\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3325 - acc: 0.8843\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2991 - acc: 0.8936\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3231 - acc: 0.8806\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2912 - acc: 0.8903\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3185 - acc: 0.8868\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2872 - acc: 0.8957\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3049 - acc: 0.8893\n",
            "Active pop:  [1, 2, 3]\n",
            "Parent:  ['flatten_5', 'dense_25', 'dense_26', 'dense_35', 'dense_30', 'dense_36']\n",
            "Child:  ['flatten_5', 'dense_37', 'dense_38', 'dense_35', 'dense_39', 'dense_40']\n",
            "19/19 [==============================] - 1s 2ms/step - loss: 0.3434 - acc: 0.8878\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3201 - acc: 0.8881\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2856 - acc: 0.8974\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3408 - acc: 0.8781\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2880 - acc: 0.8978\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3032 - acc: 0.8868\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2773 - acc: 0.8974\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2991 - acc: 0.8881\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "\n",
            "  Selected        Score       \n",
            "        T0 T1        T0   T1  \n",
            "0        1  0 -1.000000 -1.0  \n",
            "1        0  0  0.891791 -1.0  \n",
            "Task  1\n",
            "Active pop:  []\n",
            "Parent:  ['flatten_5', 'dense_32', 'dense_26', 'dense_27', 'dense_33', 'dense_34']\n",
            "{'name': 'flatten_5', 'trainable': False, 'batch_input_shape': (None, 39), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "{'name': 'flatten_41', 'trainable': False, 'batch_input_shape': (None, 20), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "(20,)\n",
            "Child:  ['flatten_41', 'dense_42', 'dense_43', 'dense_27', 'dense_44', 'dense_45']\n",
            "19/19 [==============================] - 1s 2ms/step - loss: 0.5380 - acc: 0.8183\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4510 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4324 - acc: 0.8502\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4345 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4171 - acc: 0.8506\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4268 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4076 - acc: 0.8498\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4179 - acc: 0.8445\n",
            "Active pop:  [2]\n",
            "Parent:  ['flatten_41', 'dense_42', 'dense_43', 'dense_27', 'dense_44', 'dense_45']\n",
            "Child:  ['flatten_41', 'dense_46', 'dense_43', 'dense_27', 'dense_47', 'dense_48']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.4261 - acc: 0.8493\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4296 - acc: 0.8445\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4075 - acc: 0.8498\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4078 - acc: 0.8445\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3882 - acc: 0.8502\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3801 - acc: 0.8445\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3609 - acc: 0.8535\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3475 - acc: 0.8445\n",
            "Active pop:  [2]\n",
            "Parent:  ['flatten_41', 'dense_42', 'dense_43', 'dense_27', 'dense_44', 'dense_45']\n",
            "Child:  ['flatten_41', 'dense_42', 'dense_49', 'dense_50', 'dense_51', 'dense_52']\n",
            "19/19 [==============================] - 1s 2ms/step - loss: 0.4460 - acc: 0.8481\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4305 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4243 - acc: 0.8502\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4395 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4242 - acc: 0.8502\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4288 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4204 - acc: 0.8498\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4344 - acc: 0.8445\n",
            "Active pop:  [2]\n",
            "Parent:  ['flatten_5', 'dense_32', 'dense_26', 'dense_27', 'dense_33', 'dense_34']\n",
            "{'name': 'flatten_5', 'trainable': False, 'batch_input_shape': (None, 39), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "{'name': 'flatten_53', 'trainable': False, 'batch_input_shape': (None, 20), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "(20,)\n",
            "Child:  ['flatten_53', 'dense_54', 'dense_26', 'dense_27', 'dense_33', 'dense_55']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.4998 - acc: 0.8183\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4321 - acc: 0.8433\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.4167 - acc: 0.8514\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4213 - acc: 0.8445\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3979 - acc: 0.8502\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4053 - acc: 0.8445\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3744 - acc: 0.8506\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4071 - acc: 0.8483\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "2  <keras.engine.sequential.Sequential object at ...         0.005         64   \n",
            "\n",
            "  Selected        Score            \n",
            "        T0 T1        T0        T1  \n",
            "0        1  0 -1.000000 -1.000000  \n",
            "1        0  2  0.891791 -1.000000  \n",
            "2        0  0 -1.000000  0.848259  \n",
            "Cycle  1\n",
            "Task  0\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_5', 'dense_32', 'dense_26', 'dense_27', 'dense_33', 'dense_34']\n",
            "Child:  ['flatten_5', 'dense_32', 'dense_56', 'dense_57', 'dense_33', 'dense_58']\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3720 - acc: 0.8555\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3698 - acc: 0.8632\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3092 - acc: 0.8841\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3026 - acc: 0.8794\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2819 - acc: 0.8982\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3068 - acc: 0.8831\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2802 - acc: 0.9023\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2992 - acc: 0.8881\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_5', 'dense_32', 'dense_26', 'dense_27', 'dense_33', 'dense_34']\n",
            "Child:  ['flatten_5', 'dense_59', 'dense_26', 'dense_60', 'dense_61', 'dense_62']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.3650 - acc: 0.8725\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3075 - acc: 0.8881\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2781 - acc: 0.8990\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2972 - acc: 0.8893\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2706 - acc: 0.8978\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3067 - acc: 0.8955\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2759 - acc: 0.8978\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3003 - acc: 0.8905\n",
            "Active pop:  [1, 3]\n",
            "Parent:  ['flatten_5', 'dense_32', 'dense_26', 'dense_27', 'dense_33', 'dense_34']\n",
            "Child:  ['flatten_5', 'dense_63', 'dense_26', 'dense_27', 'dense_33', 'dense_64']\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2758 - acc: 0.9002\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3105 - acc: 0.8930\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2671 - acc: 0.9015\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2980 - acc: 0.8905\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2645 - acc: 0.8994\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2978 - acc: 0.8968\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2586 - acc: 0.9052\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3086 - acc: 0.8905\n",
            "Active pop:  [1, 3, 4]\n",
            "Parent:  ['flatten_5', 'dense_59', 'dense_26', 'dense_60', 'dense_61', 'dense_62']\n",
            "Child:  ['flatten_5', 'dense_59', 'dense_26', 'dense_65', 'dense_61', 'dense_66']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.2818 - acc: 0.8982\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3039 - acc: 0.8868\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2758 - acc: 0.9019\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2986 - acc: 0.8918\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2704 - acc: 0.9036\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2971 - acc: 0.8856\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2663 - acc: 0.9015\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3006 - acc: 0.8881\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005         64   \n",
            "2  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "\n",
            "  Selected        Score            \n",
            "        T0 T1        T0        T1  \n",
            "0        1  0 -1.000000 -1.000000  \n",
            "1        0  0 -1.000000  0.848259  \n",
            "2        0  0  0.896766 -1.000000  \n",
            "Task  1\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_53', 'dense_54', 'dense_26', 'dense_27', 'dense_33', 'dense_55']\n",
            "Child:  ['flatten_53', 'dense_54', 'dense_26', 'dense_27', 'dense_33', 'dense_67']\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3625 - acc: 0.8514\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3702 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3501 - acc: 0.8518\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3691 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3460 - acc: 0.8535\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3675 - acc: 0.8470\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3430 - acc: 0.8535\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3845 - acc: 0.8495\n",
            "Active pop:  [1, 3]\n",
            "Parent:  ['flatten_53', 'dense_54', 'dense_26', 'dense_27', 'dense_33', 'dense_67']\n",
            "Child:  ['flatten_53', 'dense_54', 'dense_26', 'dense_68', 'dense_69', 'dense_70']\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4029 - acc: 0.8568\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3350 - acc: 0.8433\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3073 - acc: 0.8597\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3186 - acc: 0.8483\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2761 - acc: 0.8796\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2663 - acc: 0.8657\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2565 - acc: 0.8882\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2488 - acc: 0.8731\n",
            "Active pop:  [1, 3, 4]\n",
            "Parent:  ['flatten_53', 'dense_54', 'dense_26', 'dense_27', 'dense_33', 'dense_55']\n",
            "Child:  ['flatten_53', 'dense_71', 'dense_72', 'dense_27', 'dense_33', 'dense_73']\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4567 - acc: 0.8365\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4118 - acc: 0.8445\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3950 - acc: 0.8510\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3639 - acc: 0.8470\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3532 - acc: 0.8605\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3193 - acc: 0.8495\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3049 - acc: 0.8622\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2529 - acc: 0.8843\n",
            "Active pop:  [1, 3, 4, 5]\n",
            "Parent:  ['flatten_53', 'dense_54', 'dense_26', 'dense_27', 'dense_33', 'dense_55']\n",
            "Child:  ['flatten_53', 'dense_74', 'dense_26', 'dense_75', 'dense_33', 'dense_76']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.3588 - acc: 0.8564\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3084 - acc: 0.8644\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.2659 - acc: 0.8725\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2223 - acc: 0.8632\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1779 - acc: 0.9114\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1958 - acc: 0.8744\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1552 - acc: 0.9222\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1402 - acc: 0.9328\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "2  <keras.engine.sequential.Sequential object at ...         0.005         32   \n",
            "\n",
            "  Selected        Score            \n",
            "        T0 T1        T0        T1  \n",
            "0        1  0 -1.000000 -1.000000  \n",
            "1        0  0  0.896766 -1.000000  \n",
            "2        0  0 -1.000000  0.932836  \n",
            "Cycle  2\n",
            "Task  0\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_5', 'dense_63', 'dense_26', 'dense_27', 'dense_33', 'dense_64']\n",
            "Child:  ['flatten_5', 'dense_63', 'dense_26', 'dense_77', 'dense_78', 'dense_79']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.3074 - acc: 0.8932\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3111 - acc: 0.8893\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2773 - acc: 0.9015\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3280 - acc: 0.8794\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2724 - acc: 0.8986\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3048 - acc: 0.8881\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2700 - acc: 0.9002\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3076 - acc: 0.8893\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_5', 'dense_63', 'dense_26', 'dense_27', 'dense_33', 'dense_64']\n",
            "Child:  ['flatten_5', 'dense_80', 'dense_81', 'dense_82', 'dense_83', 'dense_84']\n",
            "10/10 [==============================] - 1s 3ms/step - loss: 0.2942 - acc: 0.8969\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3047 - acc: 0.8881\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2609 - acc: 0.9040\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3045 - acc: 0.8943\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2610 - acc: 0.9085\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3028 - acc: 0.8968\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2638 - acc: 0.9019\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.2949 - acc: 0.8918\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_53', 'dense_74', 'dense_26', 'dense_75', 'dense_33', 'dense_76']\n",
            "{'name': 'flatten_53', 'trainable': False, 'batch_input_shape': (None, 20), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "{'name': 'flatten_85', 'trainable': False, 'batch_input_shape': (None, 39), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "(39,)\n",
            "Child:  ['flatten_85', 'dense_86', 'dense_87', 'dense_88', 'dense_89', 'dense_90']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.3067 - acc: 0.8891\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3344 - acc: 0.8806\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.2904 - acc: 0.8945\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3126 - acc: 0.8856\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.2833 - acc: 0.8994\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3048 - acc: 0.8856\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.2833 - acc: 0.8949\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3163 - acc: 0.8868\n",
            "Active pop:  [1, 3]\n",
            "Parent:  ['flatten_85', 'dense_86', 'dense_87', 'dense_88', 'dense_89', 'dense_90']\n",
            "Child:  ['flatten_85', 'dense_91', 'dense_92', 'dense_88', 'dense_93', 'dense_94']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.2769 - acc: 0.8994\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3035 - acc: 0.8868\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2710 - acc: 0.8986\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3049 - acc: 0.8868\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2745 - acc: 0.8957\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3023 - acc: 0.8868\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2701 - acc: 0.9002\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3024 - acc: 0.8881\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005         32   \n",
            "2  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "\n",
            "  Selected        Score            \n",
            "        T0 T1        T0        T1  \n",
            "0        1  0 -1.000000 -1.000000  \n",
            "1        1  0 -1.000000  0.932836  \n",
            "2        2  0  0.896766 -1.000000  \n",
            "Task  1\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_53', 'dense_74', 'dense_26', 'dense_75', 'dense_33', 'dense_76']\n",
            "Child:  ['flatten_53', 'dense_95', 'dense_96', 'dense_97', 'dense_33', 'dense_98']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.1914 - acc: 0.9151\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1311 - acc: 0.9316\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1326 - acc: 0.9363\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1332 - acc: 0.9279\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1538 - acc: 0.9247\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1179 - acc: 0.9465\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1352 - acc: 0.9375\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1172 - acc: 0.9490\n",
            "Active pop:  [1, 3]\n",
            "Parent:  ['flatten_53', 'dense_74', 'dense_26', 'dense_75', 'dense_33', 'dense_76']\n",
            "Child:  ['flatten_53', 'dense_74', 'dense_99', 'dense_75', 'dense_33', 'dense_100']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.1646 - acc: 0.9300\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1424 - acc: 0.9353\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1550 - acc: 0.9193\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1253 - acc: 0.9465\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1292 - acc: 0.9350\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1350 - acc: 0.9328\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1401 - acc: 0.9317\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1270 - acc: 0.9428\n",
            "Active pop:  [1, 3, 4]\n",
            "Parent:  ['flatten_53', 'dense_74', 'dense_99', 'dense_75', 'dense_33', 'dense_100']\n",
            "Child:  ['flatten_53', 'dense_101', 'dense_99', 'dense_102', 'dense_33', 'dense_103']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.1531 - acc: 0.9276\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1356 - acc: 0.9291\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1306 - acc: 0.9342\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1417 - acc: 0.9353\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1290 - acc: 0.9334\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1091 - acc: 0.9465\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1800 - acc: 0.9127\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1182 - acc: 0.9490\n",
            "Active pop:  [1, 3, 4, 5]\n",
            "Parent:  ['flatten_53', 'dense_74', 'dense_26', 'dense_75', 'dense_33', 'dense_76']\n",
            "Child:  ['flatten_53', 'dense_104', 'dense_26', 'dense_75', 'dense_33', 'dense_105']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.1268 - acc: 0.9404\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1267 - acc: 0.9328\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1213 - acc: 0.9416\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1234 - acc: 0.9403\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1190 - acc: 0.9425\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1215 - acc: 0.9378\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1172 - acc: 0.9412\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1199 - acc: 0.9415\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "2  <keras.engine.sequential.Sequential object at ...         0.005         32   \n",
            "\n",
            "  Selected        Score            \n",
            "        T0 T1        T0        T1  \n",
            "0        1  0 -1.000000 -1.000000  \n",
            "1        2  0  0.896766 -1.000000  \n",
            "2        0  0 -1.000000  0.949005  \n",
            "Cycle  3\n",
            "Task  0\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_53', 'dense_95', 'dense_96', 'dense_97', 'dense_33', 'dense_98']\n",
            "{'name': 'flatten_53', 'trainable': False, 'batch_input_shape': (None, 20), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "{'name': 'flatten_106', 'trainable': False, 'batch_input_shape': (None, 39), 'dtype': 'float32', 'data_format': 'channels_last'}\n",
            "(39,)\n",
            "Child:  ['flatten_106', 'dense_107', 'dense_96', 'dense_97', 'dense_33', 'dense_108']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 5.4072 - acc: 0.6511\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.5078 - acc: 0.8545\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.4630 - acc: 0.8514\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.4268 - acc: 0.8532\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.4029 - acc: 0.8526\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3926 - acc: 0.8545\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.3750 - acc: 0.8535\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3764 - acc: 0.8545\n",
            "Active pop:  [1, 3]\n",
            "Parent:  ['flatten_106', 'dense_107', 'dense_96', 'dense_97', 'dense_33', 'dense_108']\n",
            "Child:  ['flatten_106', 'dense_107', 'dense_109', 'dense_110', 'dense_111', 'dense_112']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.3318 - acc: 0.8738\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3216 - acc: 0.8794\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.2973 - acc: 0.8945\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3307 - acc: 0.8794\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.2974 - acc: 0.8928\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3282 - acc: 0.8769\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.3003 - acc: 0.8882\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3165 - acc: 0.8831\n",
            "Active pop:  [1, 3, 4]\n",
            "Parent:  ['flatten_106', 'dense_107', 'dense_109', 'dense_110', 'dense_111', 'dense_112']\n",
            "Child:  ['flatten_106', 'dense_113', 'dense_114', 'dense_110', 'dense_111', 'dense_115']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.2868 - acc: 0.8982\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3126 - acc: 0.8806\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.2783 - acc: 0.9002\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3065 - acc: 0.8868\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2769 - acc: 0.9036\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3062 - acc: 0.8893\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2738 - acc: 0.9015\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3053 - acc: 0.8881\n",
            "Active pop:  [1, 3, 4, 5]\n",
            "Parent:  ['flatten_106', 'dense_113', 'dense_114', 'dense_110', 'dense_111', 'dense_115']\n",
            "Child:  ['flatten_106', 'dense_116', 'dense_114', 'dense_117', 'dense_111', 'dense_118']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.2748 - acc: 0.9011\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3043 - acc: 0.8893\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2722 - acc: 0.9007\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3111 - acc: 0.8893\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2721 - acc: 0.8986\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.3045 - acc: 0.8918\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2701 - acc: 0.9044\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3020 - acc: 0.8893\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005         32   \n",
            "2  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "\n",
            "  Selected        Score            \n",
            "        T0 T1        T0        T1  \n",
            "0        1  0 -1.000000 -1.000000  \n",
            "1        1  0 -1.000000  0.949005  \n",
            "2        2  0  0.896766 -1.000000  \n",
            "Task  1\n",
            "Active pop:  [1]\n",
            "Parent:  ['flatten_53', 'dense_95', 'dense_96', 'dense_97', 'dense_33', 'dense_98']\n",
            "Child:  ['flatten_53', 'dense_119', 'dense_120', 'dense_121', 'dense_122', 'dense_123']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.1308 - acc: 0.9354\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1169 - acc: 0.9515\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1169 - acc: 0.9450\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1216 - acc: 0.9465\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1149 - acc: 0.9404\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1131 - acc: 0.9490\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1153 - acc: 0.9458\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1195 - acc: 0.9502\n",
            "Active pop:  [1, 3]\n",
            "Parent:  ['flatten_53', 'dense_119', 'dense_120', 'dense_121', 'dense_122', 'dense_123']\n",
            "Child:  ['flatten_53', 'dense_119', 'dense_120', 'dense_124', 'dense_122', 'dense_125']\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.1150 - acc: 0.9441\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1156 - acc: 0.9465\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1122 - acc: 0.9474\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1149 - acc: 0.9490\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1113 - acc: 0.9466\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1200 - acc: 0.9366\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1117 - acc: 0.9458\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1147 - acc: 0.9428\n",
            "Active pop:  [1, 3]\n",
            "Parent:  ['flatten_53', 'dense_95', 'dense_96', 'dense_97', 'dense_33', 'dense_98']\n",
            "Child:  ['flatten_53', 'dense_95', 'dense_126', 'dense_97', 'dense_33', 'dense_127']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.1249 - acc: 0.9392\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1202 - acc: 0.9415\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.1224 - acc: 0.9371\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1244 - acc: 0.9453\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.1211 - acc: 0.9441\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1138 - acc: 0.9502\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.1139 - acc: 0.9412\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1618 - acc: 0.9291\n",
            "Active pop:  [1, 3, 4]\n",
            "Parent:  ['flatten_53', 'dense_95', 'dense_126', 'dense_97', 'dense_33', 'dense_127']\n",
            "Child:  ['flatten_53', 'dense_128', 'dense_129', 'dense_130', 'dense_131', 'dense_132']\n",
            "76/76 [==============================] - 1s 2ms/step - loss: 0.1384 - acc: 0.9321\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1360 - acc: 0.9403\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1224 - acc: 0.9425\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1222 - acc: 0.9428\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1210 - acc: 0.9338\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.1187 - acc: 0.9453\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1178 - acc: 0.9429\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1566 - acc: 0.9303\n",
            "Models (task end):                                                 Model Learning Rate Batch Size  \\\n",
            "                                                                                \n",
            "0  <keras.engine.sequential.Sequential object at ...         0.010        128   \n",
            "1  <keras.engine.sequential.Sequential object at ...         0.005        128   \n",
            "2  <keras.engine.sequential.Sequential object at ...         0.002         64   \n",
            "\n",
            "  Selected        Score            \n",
            "        T0 T1        T0        T1  \n",
            "0        1  0 -1.000000 -1.000000  \n",
            "1        2  0  0.896766 -1.000000  \n",
            "2        0  1 -1.000000  0.951493  \n"
          ]
        }
      ]
    }
  ]
}